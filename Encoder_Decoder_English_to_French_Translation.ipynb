{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder-Decoder English to French Translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5wB1pwlOaaX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM , Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64 \n",
        "epochs = 50\n",
        "latent_dim = 256\n",
        "num_samples = 8000\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_char = set()\n",
        "target_char = set()\n",
        "with open('fra.txt','r',encoding = 'utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines)-1)]:\n",
        "  input_text, target_text, _ =  line.split('\\t')\n",
        "  target_text = '\\t' + target_text + '\\n'\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  for x in input_text:\n",
        "    if x not in input_char:\n",
        "      input_char.add(x)\n",
        "  for y in target_text:\n",
        "    if y not in target_char:\n",
        "      target_char.add(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_char = sorted(list(input_char))\n",
        "target_char = sorted(list(target_char))\n",
        "size_encoder_tokens = len(input_char)\n",
        "size_decoder_tokens = len(target_char)\n",
        "max_encoder_sequential_length = max([len(y) for y in input_texts])\n",
        "max_decoder_sequential_length = max([len(x) for x in target_texts])"
      ],
      "metadata": {
        "id": "WlJ5LsXQep3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' input character unique = ', input_char)\n",
        "print(' target character unique = ', target_char)\n",
        "print(' size of the unique character in input English = ', size_encoder_tokens)\n",
        "print(' size of the unique character in target French = ', size_decoder_tokens)\n",
        "print ( \" maximum size of the sentence in input (English) = \", max_encoder_sequential_length)\n",
        "print ( \" maximum size of the sentence in output (French) = \", max_decoder_sequential_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHW6mdPXhYdH",
        "outputId": "5e9085c7-7d56-4cbb-bf33-55507ae23249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " input character unique =  [' ', '!', '\"', '$', '%', '&', \"'\", ',', '-', '.', '0', '1', '3', '5', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            " target character unique =  ['\\t', '\\n', ' ', '!', '%', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '3', '5', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'À', 'Ç', 'É', 'Ê', 'à', 'â', 'ç', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'ù', 'û', 'œ', '\\u2009', '’', '\\u202f']\n",
            " size of the unique character in input English =  69\n",
            " size of the unique character in target French =  92\n",
            " maximum size of the sentence in input (English) =  14\n",
            " maximum size of the sentence in output (French) =  59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char,i) for i, char in enumerate(input_char)])\n",
        "target_token_index = dict([(char,i) for i,char in enumerate(target_char)])"
      ],
      "metadata": {
        "id": "1iHviRxwhqbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_token_index\n",
        "# target_token_index"
      ],
      "metadata": {
        "id": "yMELy-2EmhLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_sequential_length, size_encoder_tokens), dtype = \"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_sequential_length, size_decoder_tokens), dtype = \"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "     (len(input_texts), max_decoder_sequential_length, size_decoder_tokens), dtype = \"float32\"\n",
        ")"
      ],
      "metadata": {
        "id": "XKydpxbmmpQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_text,target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i,t,input_token_index[char]] = 1\n",
        "  encoder_input_data[i,t+1:,input_token_index[' ']] = 1 \n",
        "  for t,char in enumerate(target_text):\n",
        "    decoder_input_data[i,t,target_token_index[char]] = 1\n",
        "    if t > 0:\n",
        "      decoder_target_data[i,t-1,target_token_index[char]] = 1\n",
        "\n",
        "  decoder_input_data[i,t+1:,target_token_index[' ']] = 1\n",
        "  decoder_target_data[i,t:,target_token_index[' ']] = 1"
      ],
      "metadata": {
        "id": "oyxLcWAGA6l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None, size_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "l3igh9FuFC79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, size_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(size_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "Z6t04xL9KE0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyQTX8Z9KiJ-",
        "outputId": "27f86eb6-d4c0-4c29-f2b2-591b9537c798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 9s 20ms/step - loss: 1.1590 - accuracy: 0.7408 - val_loss: 1.0137 - val_accuracy: 0.7234\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8486 - accuracy: 0.7706 - val_loss: 0.8663 - val_accuracy: 0.7573\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7024 - accuracy: 0.8088 - val_loss: 0.7250 - val_accuracy: 0.7953\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5917 - accuracy: 0.8311 - val_loss: 0.6510 - val_accuracy: 0.8107\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5359 - accuracy: 0.8434 - val_loss: 0.6113 - val_accuracy: 0.8194\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4984 - accuracy: 0.8535 - val_loss: 0.5850 - val_accuracy: 0.8281\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.4700 - accuracy: 0.8616 - val_loss: 0.5590 - val_accuracy: 0.8362\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4467 - accuracy: 0.8681 - val_loss: 0.5406 - val_accuracy: 0.8408\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4263 - accuracy: 0.8736 - val_loss: 0.5283 - val_accuracy: 0.8441\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4082 - accuracy: 0.8788 - val_loss: 0.5187 - val_accuracy: 0.8469\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3921 - accuracy: 0.8829 - val_loss: 0.5044 - val_accuracy: 0.8496\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3763 - accuracy: 0.8877 - val_loss: 0.4981 - val_accuracy: 0.8516\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3614 - accuracy: 0.8919 - val_loss: 0.4915 - val_accuracy: 0.8537\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3475 - accuracy: 0.8958 - val_loss: 0.4779 - val_accuracy: 0.8585\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3343 - accuracy: 0.8996 - val_loss: 0.4792 - val_accuracy: 0.8578\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3221 - accuracy: 0.9034 - val_loss: 0.4712 - val_accuracy: 0.8609\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3095 - accuracy: 0.9071 - val_loss: 0.4713 - val_accuracy: 0.8619\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2982 - accuracy: 0.9105 - val_loss: 0.4655 - val_accuracy: 0.8632\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2870 - accuracy: 0.9138 - val_loss: 0.4682 - val_accuracy: 0.8636\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2769 - accuracy: 0.9169 - val_loss: 0.4652 - val_accuracy: 0.8653\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2669 - accuracy: 0.9198 - val_loss: 0.4652 - val_accuracy: 0.8653\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2573 - accuracy: 0.9226 - val_loss: 0.4623 - val_accuracy: 0.8671\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2482 - accuracy: 0.9252 - val_loss: 0.4614 - val_accuracy: 0.8674\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2394 - accuracy: 0.9281 - val_loss: 0.4617 - val_accuracy: 0.8684\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2310 - accuracy: 0.9304 - val_loss: 0.4644 - val_accuracy: 0.8683\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2228 - accuracy: 0.9326 - val_loss: 0.4650 - val_accuracy: 0.8679\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2151 - accuracy: 0.9351 - val_loss: 0.4659 - val_accuracy: 0.8698\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2078 - accuracy: 0.9372 - val_loss: 0.4713 - val_accuracy: 0.8693\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2010 - accuracy: 0.9392 - val_loss: 0.4781 - val_accuracy: 0.8684\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1940 - accuracy: 0.9413 - val_loss: 0.4784 - val_accuracy: 0.8701\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1876 - accuracy: 0.9432 - val_loss: 0.4848 - val_accuracy: 0.8696\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1814 - accuracy: 0.9449 - val_loss: 0.4881 - val_accuracy: 0.8684\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1749 - accuracy: 0.9470 - val_loss: 0.4917 - val_accuracy: 0.8699\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1694 - accuracy: 0.9486 - val_loss: 0.5029 - val_accuracy: 0.8696\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1643 - accuracy: 0.9499 - val_loss: 0.5007 - val_accuracy: 0.8688\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1590 - accuracy: 0.9517 - val_loss: 0.5031 - val_accuracy: 0.8699\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1539 - accuracy: 0.9530 - val_loss: 0.5091 - val_accuracy: 0.8706\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1493 - accuracy: 0.9544 - val_loss: 0.5119 - val_accuracy: 0.8691\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1450 - accuracy: 0.9556 - val_loss: 0.5199 - val_accuracy: 0.8691\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1403 - accuracy: 0.9569 - val_loss: 0.5229 - val_accuracy: 0.8706\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1357 - accuracy: 0.9584 - val_loss: 0.5253 - val_accuracy: 0.8688\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1320 - accuracy: 0.9597 - val_loss: 0.5373 - val_accuracy: 0.8691\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1282 - accuracy: 0.9606 - val_loss: 0.5456 - val_accuracy: 0.8687\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.1247 - accuracy: 0.9617 - val_loss: 0.5428 - val_accuracy: 0.8693\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1212 - accuracy: 0.9629 - val_loss: 0.5518 - val_accuracy: 0.8688\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1175 - accuracy: 0.9638 - val_loss: 0.5595 - val_accuracy: 0.8684\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1147 - accuracy: 0.9644 - val_loss: 0.5660 - val_accuracy: 0.8680\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1115 - accuracy: 0.9652 - val_loss: 0.5688 - val_accuracy: 0.8688\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1085 - accuracy: 0.9661 - val_loss: 0.5690 - val_accuracy: 0.8686\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1056 - accuracy: 0.9669 - val_loss: 0.5778 - val_accuracy: 0.8679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef90459250>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}